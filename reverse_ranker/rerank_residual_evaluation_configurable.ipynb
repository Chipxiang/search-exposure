{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "NUM_HIDDEN_NODES = 1536\n",
    "NUM_HIDDEN_LAYERS = 1\n",
    "DROPOUT_RATE = 0.1\n",
    "CURRENT_DEVICE = \"cuda:3\"\n",
    "# REVERSE_RANKER_PATH = \"/datadrive/ruohan/rerank/train_query_50000_morepos/reverse_alpha0.5_layer1_residual1000_100_1000_0.0001_768.model\"\n",
    "# 1 layer 100 nearest neightbor: 0.7488\n",
    "# REVERSE_RANKER_PATH = \"/datadrive/ruohan/rerank/train_query_50000_morepos/reverse_alpha0.5_layer1_residual1000_100_1000_0.0001_768.model\"\n",
    "# Active Learning\n",
    "REVERSE_RANKER_PATH = \"/datadrive/ruohan/rerank/train_query_50000_morepos/reverse_alpha0.5_layer1_residual_active_learning1000_100_1000_0.0001_768.model\"\n",
    "# 1 layer 1000 nearest neighbor:\n",
    "# REVERSE_RANKER_PATH = \"/datadrive/ruohan/rerank/n_1000/reverse_alpha0.5_layer1_residual1000_100_1000_0.0001_768.model\"\n",
    "PASSAGE_NP_PATH = \"/home/jianx/results/passage_0__emb_p__data_obj_0.pb\"\n",
    "PASSAGE_MAP_PATH = \"/datadrive/jianx/data/annoy/100_ance_passage_map.dict\"\n",
    "QUERY_TRAIN_NP_PATH = \"/home/jianx/results/query_0__emb_p__data_obj_0.pb\"\n",
    "QUERY_TEST_NP_PATH = \"/home/jianx/results/test_query_0__emb_p__data_obj_0.pb\"\n",
    "QUERY_MAP_PATH = \"/datadrive/jianx/data/annoy/100_ance_query_train_map.dict\"\n",
    "RERANK_TRUE_PATH = \"/datadrive/jianx/data/results/rerank_search_rankings_100_100_flat.csv\"\n",
    "QUERY_DEV_NP_PATH = \"/home/jianx/results/dev_query_0__emb_p__data_obj_0.pb\"\n",
    "TRAIN_RERANK_PATH = \"/datadrive/jianx/data/train_data/ance_rerank_testing_rank100_nqueries50000_20000_Sep_09_19:41:09.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network\n",
    "class ResidualNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size):\n",
    "        super(ResidualNet, self).__init__()\n",
    "        \n",
    "        self.input = nn.Linear(embed_size, NUM_HIDDEN_NODES)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.normlayer = nn.LayerNorm(NUM_HIDDEN_NODES)\n",
    "        self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
    "        self.output = nn.Linear(NUM_HIDDEN_NODES, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = x\n",
    "        for i in range(NUM_HIDDEN_LAYERS):\n",
    "            out = self.input(out)\n",
    "            out = self.relu(out)\n",
    "            out = self.normlayer(out)\n",
    "            out = self.dropout(out)\n",
    "            out = self.output(out)\n",
    "            out += identity\n",
    "#             out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "    def parameter_count(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "def obj_reader(path):\n",
    "    with open(path, 'rb') as handle:\n",
    "        return pickle.load(handle, encoding=\"bytes\")\n",
    "def obj_writer(obj, path):\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# Load ground truth ranking\n",
    "def load_true_dict(k, path):\n",
    "    true_dict = {}\n",
    "    with open(path, \"r\") as file:\n",
    "        for line in file:\n",
    "            qid = int(line.split(\",\")[0])\n",
    "            pid = int(line.split(\",\")[1])\n",
    "            rank = int(line.split(\",\")[2])\n",
    "            if rank > k:\n",
    "                continue\n",
    "            if pid not in true_dict.keys():\n",
    "                true_dict[pid] = {}\n",
    "            true_dict[pid][qid] = rank\n",
    "    return true_dict\n",
    "def load_true_dict_query(k, path):\n",
    "    true_dict = {}\n",
    "    with open(path, \"r\") as file:\n",
    "        for line in file:\n",
    "            qid = int(line.split(\",\")[0])\n",
    "            pid = int(line.split(\",\")[1])\n",
    "            rank = int(line.split(\",\")[2])\n",
    "            if rank > k:\n",
    "                continue\n",
    "            if qid not in true_dict.keys():\n",
    "                true_dict[qid] = {}\n",
    "            true_dict[qid][pid] = rank\n",
    "    return true_dict\n",
    "def transform_np_transformation(query_np, b=500):\n",
    "    n = int(query_np.shape[0]/b) + 1\n",
    "\n",
    "    corpus_output = []\n",
    "    for i in range(n):\n",
    "        start = i * b\n",
    "        end = (i + 1) * b\n",
    "        if i == n-1:\n",
    "            end = query_np.shape[0]\n",
    "        q_embed = query_np[start:end,:]\n",
    "        q_embed = torch.from_numpy(q_embed).to(CURRENT_DEVICE)\n",
    "        corpus_output.append(reverse_ranker(q_embed).detach().cpu().numpy())\n",
    "    corpus_np = np.concatenate(corpus_output[:-1])\n",
    "    corpus_np = np.concatenate((corpus_np, corpus_output[-1]))\n",
    "    print(corpus_np.shape)\n",
    "    return corpus_np\n",
    "# 6. Find n nearest queries of a passage\n",
    "# 7. Compare with the groud truth\n",
    "def evaluate_reverse_ranker(pred_rank, true_rank, k = 100):\n",
    "    top_true = []\n",
    "    top_pred = []\n",
    "    for pid, qids in pred_rank.items():\n",
    "        n_top_true = len(true_rank.get(pid, {}))\n",
    "        temp_pred = np.fromiter(qids.values(), dtype=int)\n",
    "        n_top_pred = sum((temp_pred != 0) & (temp_pred <= k))\n",
    "        top_true.append(n_top_true)\n",
    "        top_pred.append(n_top_pred)\n",
    "    return top_true, top_pred\n",
    "\n",
    "def generate_pred_rank(query_index, true_dict, baseline_dict, passage_embed, \n",
    "                       qid_mapping, pid_reverse_mapping, n=100, k=100):\n",
    "    pid_list = list(baseline_dict.keys())\n",
    "    p_embed_list = []\n",
    "    all_results = {}\n",
    "    print(\"Begin append.\")\n",
    "    for i, pid in enumerate(pid_list):\n",
    "        if i >= n:\n",
    "            break\n",
    "        pid_r = pid_reverse_mapping[pid]\n",
    "        p_embed = np.array(passage_embed[pid_r])\n",
    "        p_embed_list.append(p_embed)\n",
    "    p_embed_all = np.stack(p_embed_list)\n",
    "    print(\"Finish append.\")\n",
    "    print(\"Begin search.\")\n",
    "    _, near_qids = query_index.search(p_embed_all, k)\n",
    "    print(\"Finish search.\")\n",
    "    for i, pid in enumerate(pid_list):\n",
    "        temp_results = {}\n",
    "        for qid in near_qids[i]:\n",
    "            qid = qid_mapping[qid]\n",
    "            try:\n",
    "                rank = true_dict[pid][qid]\n",
    "            except:\n",
    "                rank = 0\n",
    "            temp_results[qid] = rank\n",
    "        all_results[pid] = temp_results\n",
    "    return all_results\n",
    "\n",
    "N_PASSAGE = 100\n",
    "TRAIN_PASSAGE = 200000\n",
    "def load_train(path, N_PASSAGE, TRAIN_PASSAGE):\n",
    "    with open(path) as file:\n",
    "        my_dict = {}\n",
    "        count = 0\n",
    "        for line in file:\n",
    "            count += 1\n",
    "            if count <= TRAIN_PASSAGE * 100:\n",
    "                continue            \n",
    "            if count > (TRAIN_PASSAGE + N_PASSAGE) * 100:\n",
    "                break\n",
    "            tokens = line.split(\",\")\n",
    "            pid = int(tokens[0])\n",
    "            qid = int(tokens[1])\n",
    "            rank = int(tokens[2].rstrip())\n",
    "            if pid not in my_dict:\n",
    "                my_dict[pid] = {}\n",
    "            my_dict[pid][qid] = rank\n",
    "    return my_dict\n",
    "def load_train_dict(path):\n",
    "    with open(path, \"r\") as file:\n",
    "        pos_dict = {}\n",
    "        neg_dict = {}\n",
    "        count = 0\n",
    "        for line in file:\n",
    "            tokens = line.split(\",\")\n",
    "            pid = int(tokens[0])\n",
    "            qid = int(tokens[1])\n",
    "            rank = int(tokens[2].rstrip())\n",
    "            if rank == 0:\n",
    "                if pid not in neg_dict:\n",
    "                    neg_dict[pid] = {}\n",
    "                neg_dict[pid][qid] = 200\n",
    "            else:\n",
    "                if pid not in pos_dict:\n",
    "                    pos_dict[pid] = {}\n",
    "                pos_dict[pid][qid] = rank\n",
    "    return pos_dict, neg_dict\n",
    "def count_unique_queries(train_pos, train_neg):\n",
    "    unique_queries = set()\n",
    "    for pid, qids in train_pos.items():\n",
    "        for qid, rank in qids.items():\n",
    "            unique_queries.add(qid)\n",
    "    for pid, qids in train_neg.items():\n",
    "        for qid, rank in qids.items():\n",
    "            unique_queries.add(qid)\n",
    "    print(len(unique_queries))\n",
    "    return unique_queries\n",
    "def compare_with_baseline(query_index, true_dict_100, forward_baseline_rank, passage_embed, \n",
    "                          qid_mapping, pid_reverse_mapping,n):\n",
    "    pred_rank = generate_pred_rank(query_index, true_dict_100, forward_baseline_rank, \n",
    "                                   passage_embed, qid_mapping, pid_reverse_mapping, n=n)\n",
    "    top_true, top_pred = evaluate_reverse_ranker(pred_rank, true_dict_100, k = 100)\n",
    "    print(\"New model: {}\".format(np.mean(top_pred)/np.mean(top_true)))\n",
    "    top_true_baseline, top_pred_baseline = evaluate_reverse_ranker(forward_baseline_rank, true_dict_100, k = 100)\n",
    "    print(\"Baseline model: {}\".format(np.mean(top_pred_baseline)/np.mean(top_true_baseline)))\n",
    "    return top_true, top_pred, top_true_baseline, top_pred_baseline, pred_rank\n",
    "def delete_zeros(myDict):\n",
    "    out_dict = {key:val for key, val in myDict.items() if val != 0}\n",
    "    return set(list(out_dict.keys()))\n",
    "def compare_specific_passage(pred_rank_test1, forward_baseline_rank_test1, n):\n",
    "    count = 0\n",
    "    count_loss = 0\n",
    "    all_count = 0\n",
    "    all_count_loss = 0\n",
    "    for i in range(n):\n",
    "        pid = list(pred_rank_test1.keys())[i]\n",
    "        pred = delete_zeros(pred_rank_test1[pid])\n",
    "        baseline = delete_zeros(forward_baseline_rank_test1[pid])\n",
    "        diff = pred - baseline\n",
    "        if diff != set():\n",
    "            count += 1\n",
    "            all_count += len(diff)\n",
    "#             print(\"Newly found {}\".format(diff))\n",
    "        diff_loss = baseline - pred\n",
    "        if diff_loss != set():\n",
    "            count_loss += 1\n",
    "            all_count_loss += len(diff_loss)\n",
    "#             print(\"Lose {}\".format(diff_loss))\n",
    "    print(\"Percentage of passages with newly found exposing queries: {}\".format(count/n))\n",
    "    print(\"Percentage of passages that lose originaly found exposing queries: {}\".format(count_loss/n))\n",
    "    print(\"Number of newly found exposing queries:{} Number of lost exposing queries:{} Net gain:{}\".format(all_count, all_count_loss, all_count - all_count_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResidualNet(\n",
       "  (input): Linear(in_features=768, out_features=1536, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (normlayer): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (output): Linear(in_features=1536, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(REVERSE_RANKER_PATH)\n",
    "reverse_ranker = ResidualNet(embed_size=768)\n",
    "reverse_ranker.load_state_dict(checkpoint['model'])\n",
    "reverse_ranker.to(CURRENT_DEVICE)\n",
    "reverse_ranker.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load passages.\n",
      "Load queries.\n",
      "Load pre-processed results.\n"
     ]
    }
   ],
   "source": [
    "print(\"Load passages.\")\n",
    "passage_np = obj_reader(PASSAGE_NP_PATH)\n",
    "pid_mapping = obj_reader(PASSAGE_MAP_PATH)\n",
    "print(\"Load queries.\")\n",
    "query_np = obj_reader(QUERY_TRAIN_NP_PATH)\n",
    "qid_mapping = obj_reader(QUERY_MAP_PATH)\n",
    "print(\"Load pre-processed results.\")\n",
    "true_rerank_dict_100 = load_true_dict(100, RERANK_TRUE_PATH)\n",
    "pid_reverse_mapping = {v: k for k, v in pid_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(502939, 768)\n",
      "(8841823, 768)\n"
     ]
    }
   ],
   "source": [
    "query_new_np = transform_np_transformation(query_np)\n",
    "passage_new_np = transform_np_transformation(passage_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Add the new numpy array to Flatindex\n",
    "dim = query_new_np.shape[1]\n",
    "query_index = faiss.IndexFlatIP(dim)\n",
    "query_index.add(query_new_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on 1000 nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin append.\n",
      "Finish append.\n",
      "Begin search.\n",
      "Finish search.\n",
      "New model: 0.716904768535279\n",
      "Baseline model: 0.6476649307972931\n",
      "Percentage of passages with newly found exposing queries: 0.37325\n",
      "Percentage of passages that lose originaly found exposing queries: 0.20745\n",
      "Number of newly found exposing queries:13866 Number of lost exposing queries:6407 Net gain:7459\n"
     ]
    }
   ],
   "source": [
    "forward_baseline_rank_test_rerank = load_train(TRAIN_RERANK_PATH, N_PASSAGE = 20000, TRAIN_PASSAGE = 0)\n",
    "top_true_test_rerank, top_pred_test_rerank, top_true_baseline_test_rerank, top_pred_baseline_test_rerank, pred_rank_test_rerank = compare_with_baseline(query_index, true_rerank_dict_100, forward_baseline_rank_test_rerank, passage_new_np, qid_mapping, pid_reverse_mapping, n=20000)\n",
    "compare_specific_passage(pred_rank_test_rerank, forward_baseline_rank_test_rerank, n=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49769\n",
      "49769 queries have been seen by the reverse ranker during training.\n",
      "393902\n",
      "393902\n",
      "354228/393902 of the exposing queries have not seen by the reverse ranker while training.\n"
     ]
    }
   ],
   "source": [
    "train_pos_dict, train_neg_dict = load_train_dict(\"/datadrive/jianx/data/train_data/ance_rerank_training_rank100_nqueries50000_200000_Sep_09_19:41:09.csv\")\n",
    "unique_queries = count_unique_queries(train_pos_dict, train_neg_dict)\n",
    "print(\"{} queries have been seen by the reverse ranker during training.\".format(len(unique_queries)))\n",
    "expose = count_unique_queries(pred_rank_test_rerank, pred_rank_test_rerank)\n",
    "expose_append = count_unique_queries(pred_rank_test_rerank, pred_rank_test_rerank)\n",
    "print(\"{}/{} of the exposing queries have not seen by the reverse ranker while training.\".format(len(expose_append\n",
    "       - unique_queries), len(expose_append)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on 100 nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin append.\n",
      "Finish append.\n",
      "Begin search.\n",
      "Finish search.\n",
      "New model: 0.7488094906569385\n",
      "Baseline model: 0.6476649307972931\n",
      "Percentage of passages with newly found exposing queries: 0.4022\n",
      "Percentage of passages that lose originaly found exposing queries: 0.16355\n",
      "Number of newly found exposing queries:15758 Number of lost exposing queries:4862 Net gain:10896\n"
     ]
    }
   ],
   "source": [
    "forward_baseline_rank_test_rerank = load_train(TRAIN_RERANK_PATH, N_PASSAGE = 20000, TRAIN_PASSAGE = 0)\n",
    "top_true_test_rerank, top_pred_test_rerank, top_true_baseline_test_rerank, top_pred_baseline_test_rerank, pred_rank_test_rerank = compare_with_baseline(query_index, true_rerank_dict_100, forward_baseline_rank_test_rerank, passage_new_np, qid_mapping, pid_reverse_mapping, n=20000)\n",
    "compare_specific_passage(pred_rank_test_rerank, forward_baseline_rank_test_rerank, n=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49769\n",
      "49769 queries have been seen by the reverse ranker during training.\n",
      "410181\n",
      "410181\n",
      "368930/410181 of the exposing queries have not seen by the reverse ranker while training.\n"
     ]
    }
   ],
   "source": [
    "train_pos_dict, train_neg_dict = load_train_dict(\"/datadrive/jianx/data/train_data/ance_rerank_training_rank100_nqueries50000_200000_Sep_09_19:41:09.csv\")\n",
    "unique_queries = count_unique_queries(train_pos_dict, train_neg_dict)\n",
    "print(\"{} queries have been seen by the reverse ranker during training.\".format(len(unique_queries)))\n",
    "expose = count_unique_queries(pred_rank_test_rerank, pred_rank_test_rerank)\n",
    "expose_append = count_unique_queries(pred_rank_test_rerank, pred_rank_test_rerank)\n",
    "print(\"{}/{} of the exposing queries have not seen by the reverse ranker while training.\".format(len(expose_append\n",
    "       - unique_queries), len(expose_append)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin append.\n",
      "Finish append.\n",
      "Begin search.\n",
      "Finish search.\n",
      "New model: 0.7592804032415272\n",
      "Baseline model: 0.6476649307972931\n",
      "Percentage of passages with newly found exposing queries: 0.4078\n",
      "Percentage of passages that lose originaly found exposing queries: 0.1403\n",
      "Number of newly found exposing queries:16101 Number of lost exposing queries:4077 Net gain:12024\n"
     ]
    }
   ],
   "source": [
    "forward_baseline_rank_test_rerank = load_train(TRAIN_RERANK_PATH, N_PASSAGE = 20000, TRAIN_PASSAGE = 0)\n",
    "top_true_test_rerank, top_pred_test_rerank, top_true_baseline_test_rerank, top_pred_baseline_test_rerank, pred_rank_test_rerank = compare_with_baseline(query_index, true_rerank_dict_100, forward_baseline_rank_test_rerank, passage_new_np, qid_mapping, pid_reverse_mapping, n=20000)\n",
    "compare_specific_passage(pred_rank_test_rerank, forward_baseline_rank_test_rerank, n=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49769\n",
      "49769 queries have been seen by the reverse ranker during training.\n",
      "419111\n",
      "419111\n",
      "376621/419111 of the exposing queries have not seen by the reverse ranker while training.\n"
     ]
    }
   ],
   "source": [
    "train_pos_dict, train_neg_dict = load_train_dict(\"/datadrive/jianx/data/train_data/ance_rerank_training_rank100_nqueries50000_200000_Sep_09_19:41:09.csv\")\n",
    "unique_queries = count_unique_queries(train_pos_dict, train_neg_dict)\n",
    "print(\"{} queries have been seen by the reverse ranker during training.\".format(len(unique_queries)))\n",
    "expose = count_unique_queries(pred_rank_test_rerank, pred_rank_test_rerank)\n",
    "expose_append = count_unique_queries(pred_rank_test_rerank, pred_rank_test_rerank)\n",
    "print(\"{}/{} of the exposing queries have not seen by the reverse ranker while training.\".format(len(expose_append\n",
    "       - unique_queries), len(expose_append)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49769\n",
      "49769 queries have been seen by the reverse ranker during training.\n",
      "192729\n",
      "192729\n",
      "173545/192729 of the exposing queries have not seen by the reverse ranker while training.\n"
     ]
    }
   ],
   "source": [
    "train_pos_dict, train_neg_dict = load_train_dict(\"/datadrive/jianx/data/train_data/ance_rerank_training_rank100_nqueries50000_200000_Sep_09_19:41:09.csv\")\n",
    "unique_queries = count_unique_queries(train_pos_dict, train_neg_dict)\n",
    "print(\"{} queries have been seen by the reverse ranker during training.\".format(len(unique_queries)))\n",
    "expose = count_unique_queries(pred_rank_test_rerank, pred_rank_test_rerank)\n",
    "expose_append = count_unique_queries(pred_rank_test_rerank, pred_rank_test_rerank)\n",
    "print(\"{}/{} of the exposing queries have not seen by the reverse ranker while training.\".format(len(expose_append\n",
    "       - unique_queries), len(expose_append)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_PATH = \"/datadrive/ruohan/reverse_ranker/residual/train_query_50000_morepos_layer1_reludel/\"\n",
    "obj_writer(forward_baseline_rank_test_rerank, RESULT_PATH + \"forward_baseline_rank_test.pickle\")\n",
    "obj_writer(top_true_baseline_test_rerank, RESULT_PATH + \"top_true_test.pickle\")\n",
    "obj_writer(top_pred_test_rerank, RESULT_PATH + \"top_pred_test.pickle\")\n",
    "obj_writer(top_true_baseline_test_rerank, RESULT_PATH + \"top_true_baseline_test.pickle\")\n",
    "obj_writer(top_pred_baseline_test_rerank, RESULT_PATH + \"top_pred_baseline_test.pickle\")\n",
    "obj_writer(pred_rank_test_rerank, RESULT_PATH + \"pred_rank_test.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_query_dict = load_true_dict_query(100, RERANK_TRUE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_dict, train_neg_dict = load_train_dict(\"/datadrive/jianx/data/train_data/ance_rerank_training_rank100_nqueries50000_200000_Sep_09_19:41:09.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages_list = list(train_neg_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages_list_wrong = [pid_reverse_mapping[i] for i in passages_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_mapping = obj_reader(QUERY_MAP_PATH)\n",
    "queries_list = [qid_mapping[i] for i in range(50000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids_set = set(passages_list)\n",
    "for qid in queries_list:\n",
    "    temp_dict = true_query_dict.get(qid, {})\n",
    "    if temp_dict == {}:\n",
    "        continue\n",
    "    temp_list = list(temp_dict.keys())\n",
    "    temp_list = list(set(temp_list) & pids_set)\n",
    "    for pid in temp_list:\n",
    "        if pid not in train_pos_dict:\n",
    "            train_pos_dict[pid] = {}\n",
    "        train_pos_dict[pid][qid] = temp_dict[pid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199541"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_writer(train_pos_dict, \"/datadrive/ruohan/data/ance_rerank_training_rank100_nqueries50000_200000_Sep_09_19:41:09.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{819199: 95}\n",
      "{685143: 42}\n",
      "{1146692: 36}\n",
      "{4882: 13, 449760: 90}\n",
      "{528374: 82}\n",
      "{1184718: 17}\n",
      "{208476: 8}\n",
      "{908613: 41}\n",
      "{876069: 69}\n",
      "{625050: 43}\n",
      "{52094: 13, 848198: 36, 46066: 64}\n",
      "{698422: 38}\n",
      "{100672: 83}\n",
      "{35927: 11}\n",
      "{317681: 75, 49309: 62}\n",
      "{1034791: 34}\n",
      "{818920: 8, 598248: 84, 187194: 58}\n",
      "{587725: 25}\n",
      "{314854: 13, 679448: 74}\n",
      "{44940: 89}\n",
      "{610096: 6}\n",
      "{92138: 29}\n",
      "{594595: 43}\n",
      "{406672: 75, 522164: 66}\n",
      "{880291: 79}\n",
      "{806193: 42}\n",
      "{778438: 10}\n",
      "{744260: 6, 721554: 36}\n",
      "{101778: 47, 316768: 10, 1151400: 2, 307312: 61}\n",
      "{220349: 57}\n",
      "{378102: 66}\n",
      "{211259: 15, 965609: 58}\n",
      "{932358: 18}\n",
      "{375722: 11, 255082: 99}\n",
      "{1157681: 94, 697642: 93}\n",
      "{917168: 14, 1022758: 20}\n",
      "{1043987: 38, 367571: 50}\n",
      "{1144744: 22}\n",
      "{819979: 63}\n",
      "{664458: 25}\n",
      "{86270: 63}\n",
      "{209704: 41}\n",
      "{321825: 14, 436538: 24}\n",
      "{903066: 21}\n",
      "{941879: 60, 336724: 94}\n",
      "{208833: 94}\n",
      "{770138: 40, 1023556: 82}\n",
      "{511782: 21, 200375: 22, 172932: 62}\n",
      "{208614: 68}\n",
      "{1007897: 29, 608398: 50}\n",
      "{742580: 74}\n",
      "{576345: 90}\n",
      "{135915: 23}\n",
      "{660379: 11, 99700: 43, 660587: 68}\n",
      "{574473: 61}\n",
      "{334796: 29, 1055021: 92, 1055019: 63, 1026518: 99}\n",
      "{565972: 9, 98000: 13, 803089: 34, 499240: 27, 542589: 48, 508384: 56, 1172275: 39, 188259: 68, 1150219: 98, 574301: 92, 89335: 78, 507980: 59, 439459: 99}\n",
      "{564459: 49}\n",
      "{688871: 4, 629002: 85, 1032794: 28}\n",
      "{212475: 45, 990139: 81}\n",
      "{723239: 30}\n",
      "{121623: 60}\n",
      "{817037: 87, 884508: 52}\n",
      "{1014563: 91}\n",
      "{1040435: 62, 1040433: 83}\n",
      "{117089: 91, 555048: 100}\n",
      "{147686: 8, 762076: 43, 1075040: 41}\n",
      "{100514: 20, 86335: 44}\n",
      "{673591: 8}\n",
      "{812324: 72}\n",
      "{953950: 74}\n",
      "{608612: 37, 29541: 47, 610411: 63, 1141067: 97, 611529: 91, 606614: 85}\n",
      "{290021: 39}\n",
      "{551178: 60, 845357: 95, 361048: 86}\n",
      "{362253: 70, 718326: 15}\n",
      "{1169087: 46}\n",
      "{1060953: 26}\n",
      "{1175869: 4}\n",
      "{837598: 48}\n",
      "{35842: 60}\n",
      "{777020: 45}\n",
      "{289377: 15}\n",
      "{712910: 100}\n",
      "{199862: 70}\n",
      "{1064539: 40}\n",
      "{1035400: 71}\n",
      "{153420: 15, 231272: 33, 153506: 95, 231254: 42, 175976: 35, 256314: 68}\n",
      "{671271: 76}\n",
      "{642892: 6, 854453: 22, 826439: 31, 727526: 36}\n",
      "{981932: 89, 1007773: 63}\n",
      "{884187: 76}\n",
      "{182797: 60}\n",
      "{1184802: 31}\n",
      "{785442: 22, 606438: 16}\n",
      "{465008: 4}\n",
      "{398672: 97}\n",
      "{1022194: 55, 85574: 76}\n",
      "{694353: 56}\n",
      "{415650: 21, 887374: 86}\n",
      "{118094: 21}\n",
      "{431967: 9, 487202: 46, 632240: 90}\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for pid, qids in train_pos_dict.items():\n",
    "    if len(qids) > 100:\n",
    "        print(qids)\n",
    "    if len(qids) < 100:\n",
    "        print(qids)\n",
    "    count += 1\n",
    "    if count > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data for forward ranker\n",
    "# to learn the BM25 filtering\n",
    "true_full_query_100 = load_true_dict_query(100, \"/datadrive/jianx/data/results/all_search_rankings_100_100_flat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1706"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true_full_query_100) - len(true_query_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/datadrive/ruohan/data/rerank_learnBM25.csv\",'w') as f:\n",
    "    for qid in queries_list:\n",
    "        if qid not in true_query_dict:\n",
    "            continue\n",
    "        pids = list(true_full_query_100[qid].keys())\n",
    "        for pid in pids:\n",
    "            rank = true_query_dict[qid].get(pid, 0)\n",
    "            f.write('{},{},{}\\n'.format(qid, pid, rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/datadrive/ruohan/data/rerank_learnBM25_morepos.csv\",'w') as f:\n",
    "    for qid in queries_list:\n",
    "        if qid not in true_query_dict:\n",
    "            continue\n",
    "        pids = list(true_full_query_100[qid].keys())\n",
    "        temp_dict = true_query_dict[qid]\n",
    "        out_pos_pids = list(set(list(temp_dict.keys()))-set(pids))\n",
    "        for pid in pids:\n",
    "            rank = temp_dict.get(pid, 0)\n",
    "            f.write('{},{},{}\\n'.format(qid, pid, rank))\n",
    "        for pidout in out_pos_pids:\n",
    "            rankout = temp_dict.get(pidout)\n",
    "            f.write('{},{},{}\\n'.format(qid, pidout, rankout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_index(obj_np):\n",
    "    dim = obj_np.shape[1]\n",
    "    out_index = faiss.IndexFlatIP(dim)\n",
    "    out_index.add(obj_np)\n",
    "    return out_index\n",
    "def generate_ground_truth_true_id(out_index, test_np, qid_mapping, pid_mapping, passages_list, true_dict, k=100):\n",
    "    _, near_pids = out_index.search(test_np, k)\n",
    "    results = {}\n",
    "    for i, pid in enumerate(passages_list):\n",
    "        if pid not in results:\n",
    "            results[pid] = {}\n",
    "        for qid in near_pids[i,:]:\n",
    "            qid_true = qid_mapping[qid]\n",
    "            try:\n",
    "                rank = true_dict[qid_true][pid]\n",
    "            except:\n",
    "                rank = 0\n",
    "            if rank > 0:\n",
    "                print(rank)\n",
    "            results[pid][qid_true] = rank\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_partial_index = generate_index(query_np[:50000,:])\n",
    "passage_selected_np = passage_np[passages_list_wrong,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n",
      "41\n",
      "72\n",
      "34\n",
      "90\n",
      "21\n",
      "64\n",
      "19\n",
      "41\n",
      "63\n",
      "56\n",
      "6\n",
      "64\n",
      "59\n",
      "63\n",
      "97\n",
      "63\n",
      "89\n",
      "8\n",
      "79\n",
      "89\n",
      "84\n",
      "37\n",
      "45\n",
      "82\n",
      "7\n",
      "35\n",
      "39\n",
      "47\n",
      "67\n",
      "1\n",
      "32\n",
      "92\n",
      "67\n",
      "44\n",
      "86\n",
      "61\n",
      "62\n",
      "54\n",
      "32\n",
      "34\n",
      "77\n",
      "1\n",
      "38\n",
      "2\n",
      "7\n",
      "17\n",
      "27\n",
      "89\n",
      "93\n",
      "93\n",
      "6\n",
      "50\n",
      "73\n",
      "67\n",
      "88\n",
      "29\n",
      "78\n",
      "9\n",
      "88\n",
      "17\n",
      "12\n",
      "33\n",
      "44\n",
      "44\n",
      "44\n",
      "19\n",
      "75\n",
      "21\n",
      "90\n",
      "14\n",
      "25\n",
      "59\n",
      "62\n",
      "86\n",
      "62\n",
      "9\n",
      "16\n",
      "72\n",
      "59\n",
      "56\n",
      "20\n",
      "89\n",
      "35\n",
      "70\n",
      "89\n",
      "79\n",
      "35\n",
      "79\n",
      "33\n",
      "81\n",
      "25\n",
      "93\n",
      "83\n",
      "64\n",
      "28\n",
      "55\n",
      "16\n",
      "78\n",
      "97\n",
      "78\n",
      "36\n",
      "29\n",
      "88\n",
      "87\n",
      "21\n",
      "16\n",
      "44\n",
      "97\n",
      "91\n",
      "34\n",
      "23\n",
      "51\n",
      "26\n",
      "17\n",
      "69\n",
      "82\n",
      "100\n",
      "51\n",
      "68\n",
      "86\n",
      "10\n",
      "88\n",
      "9\n",
      "91\n",
      "83\n",
      "46\n",
      "78\n",
      "22\n",
      "34\n",
      "50\n",
      "71\n",
      "63\n",
      "51\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "train_data_1000 = generate_ground_truth_true_id(query_partial_index, passage_selected_np, qid_mapping, pid_mapping,\n",
    "                                                passages_list,true_rerank_dict_100,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/datadrive/ruohan/data/rerank_top1000_train.csv\",'w') as f:\n",
    "    for qid, results in train_data_1000.items():\n",
    "        for pid, rank in results.items():\n",
    "            f.write('{},{},{}\\n'.format(pid, qid, rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data from current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin append.\n",
      "Finish append.\n",
      "Begin search.\n",
      "Finish search.\n",
      "New model: 0.7330567000369039\n",
      "Baseline model: 0.1340413618659141\n",
      "Percentage of passages with newly found exposing queries: 0.92135\n",
      "Percentage of passages that lose originaly found exposing queries: 0.11285\n",
      "Number of newly found exposing queries:125691 Number of lost exposing queries:2844 Net gain:122847\n"
     ]
    }
   ],
   "source": [
    "forward_baseline_rank_test_rerank = load_train(\"/datadrive/jianx/data/train_data/ance_rerank_training_rank100_nqueries50000_200000_Sep_09_19:41:09.csv\", N_PASSAGE = 200000, TRAIN_PASSAGE = 0)\n",
    "top_true_test_rerank, top_pred_test_rerank, top_true_baseline_test_rerank, top_pred_baseline_test_rerank, pred_rank_test_rerank = compare_with_baseline(query_index, true_rerank_dict_100, forward_baseline_rank_test_rerank, passage_new_np, qid_mapping, pid_reverse_mapping, n=200000)\n",
    "compare_specific_passage(pred_rank_test_rerank, forward_baseline_rank_test_rerank, n=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_query_index = generate_index(query_new_np[:50000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_true_list = list(pred_rank_test_rerank.keys())\n",
    "p_fake_list = [pid_reverse_mapping[i] for i in p_true_list]\n",
    "new_train_passages = passage_new_np[p_fake_list,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, new_near = new_query_index.search(new_train_passages, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_writer(query_new_np, \"/datadrive/ruohan/data/active_query_np.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_writer(passage_new_np, \"/datadrive/ruohan/data/active_passage_np.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37_pytorch]",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
